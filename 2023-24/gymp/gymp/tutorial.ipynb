{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A simple tutorial for the main functionalities of `gymnasium`. For more, check the `gymnasium` [documentation](https://gymnasium.farama.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments\n",
    "Gym MDPs are implemented though the class `Environment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(gym.envs.registry.keys())[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment interaction\n",
    "Environment interaction follows the following scheme\n",
    "![](./imgs/env.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")#, render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Render options**\n",
    "- ``None`` (default): no render is computed.\n",
    "- ``human``: The environment is continuously rendered in the current display or terminal, usually for human consumption. This rendering should occur during step() and render() doesn’t need to be called. Returns None.\n",
    "- ``rgb_array``: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n",
    "- ``ansi``: Return a strings (str) or StringIO.StringIO containing a terminal-style text representation for each time step. The text can include newlines and ANSI escape sequences (e.g. for colors).\n",
    "- ``rgb_array_list`` and ``ansi_list``: List based version of render modes are possible (except Human) \n",
    "through the wrapper, ``gymnasium.wrappers``. \n",
    "``RenderCollection`` that is automatically applied during `gymnasium.make(..., render_mode=\"rgb_array_list\")`. \n",
    "The frames collected are popped after `render()` is called or `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
    "observation, info = env.reset()\n",
    "for i in range(200):\n",
    "    if i < 3:\n",
    "        print(env.render())\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation, reward and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENTS = {\n",
    "    'lake': lambda mode: gym.make(\"FrozenLake-v1\", render_mode=mode),\n",
    "    'lunar': lambda mode: gym.make(\"LunarLander-v2\", render_mode=mode),\n",
    "    'car': lambda mode: gym.make(\"CarRacing-v2\", render_mode=mode)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space options\n",
    "- `Box`: describes an n-dimensional continuous space. It’s a bounded space where we can define the upper and lower limits which describe the valid values our observations can take.\n",
    "- `Discrete`: describes a discrete space where ${0, 1, \\dots, n-1}$ are the possible values our observation or action can take. Values can be shifted to ${a, a+1, \\dots, a+n-1}$ using an optional argument.\n",
    "- `Dict`: represents a dictionary of simple spaces.\n",
    "- `Tuple`: represents a tuple of simple spaces.\n",
    "- `MultiBinary`: creates an n-shape binary space. Argument n can be a number or a list of numbers.\n",
    "- `MultiDiscrete`: consists of a series of Discrete action spaces with a different number of actions in each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ENVIRONMENTS['car'](None)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'car'\n",
    "env = ENVIRONMENTS[example](None)\n",
    "observation, info = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(observation, reward, terminated, truncated, info)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use wrappers to modify existing environments\n",
    "- `TimeLimit`: Issue a truncated signal if a maximum number of timesteps has been exceeded (or the base environment has issued a truncated signal).\n",
    "- `ClipAction`: Clip the action such that it lies in the action space (of type Box).\n",
    "- `RescaleAction`: Rescale actions to lie in a specified interval\n",
    "- `TimeAwareObservation`: Add information about the index of timestep to observation. In some cases helpful to ensure that transitions are Markov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ENVIRONMENTS['car'](None)\n",
    "print(env.observation_space.shape)\n",
    "wrap_env = FlattenObservation(env)\n",
    "print(wrap_env.observation_space.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
