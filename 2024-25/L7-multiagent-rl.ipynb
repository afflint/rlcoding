{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Universit√† degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# Multi-agent reinforcement learning\n",
    "\n",
    "### Luigi Foscari\n",
    "\n",
    "<small>see sections 1.1 to 1.4 and 5.2 to 5.4 of [Multi-Agent Reinforcement Learning: Foundations and Modern Approaches](https://www.marl-book.com) for a more in-depth overview. Consider watching this [YouTube video](https://www.youtube.com/watch?v=QfYx5q0Q75M) by the book's author which provides a great introduction.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we tackled the problem of learning an optimal policy for a single agent inside a MDP, we now shift our focus towards contexts in which there are _multiple_ agents acting in the MDP with (possibly) different goals. For example, consider a fleet of autonomous vehicles with the goal of driving as quickly and safely as possible, a team of players in a competitive videogame or a group of automated traders that manage each their own investments in a public market.\n",
    "\n",
    "**How does the addition of other learning agents change what we learned so far?**\n",
    "\n",
    "_See the available [environments](https://pettingzoo.farama.org/environments/classic/) from the PettinZoo library for more examples_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **multi-agent system** is composed of an environment and multiple decision-making agents, also called _players_, that interact in the environment to achieve certain goals. The environment evolves during play according the player's actions and optionally randomness. \n",
    "\n",
    "**Multi-agent reinforcement learning** or **MARL** tackles learning in such systems by devising algorithms that let the players refine their respective strategies, which can be collaborative, competitive or a mix of the two.\n",
    "\n",
    "At the start of a round of the game, each player sees the state of the environment and picks an action, the round ends after every player has chosen an action, then the envoriment evolves accordingly and each player collects their reward to update their respective policy.\n",
    "\n",
    "<img src=\"imgs/environment-marl.png\" width=\"600\" style=\"display: block; margin: 0 auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the point of view of the agent, little has changed, they still have a set of actions, a policy, an enviroment and the goal of optimizing their policy.\n",
    "What changed? The difference lies in the fact that **the immediate reward achieved by any player depends on the actions of the other players on the same round**.\n",
    "\n",
    "For example consider a group of learning agents playing football, when a goal is scored, the success is tied to the actions of all the players, not only of the player that last kicked the ball. This introduces the problem of understanding how much of each player contributed, this is called **multi-agent credit assignment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue that arises in MARL is the dependence on the number of agents, when considering games with a thousands of players, our learning procedure might struggle. When dealing with multiple agents, the algorithms will need to scale well with the number of agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central learning\n",
    "One natural approach to the problem of learning with multiple agents is to use a single learning procedure (e.g. Q-learning) which learns the best strategy for each player, like a conductor in an orchestra, we shall call this algorithm the _maestro_. If each player $i \\in \\{1, \\dots, N\\}$ has an action space $\\mathcal{A}_i$, then the maestro will have action space $\\mathcal{A} = \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_N$, the state space is $\\mathcal{S}$ and on each round the maestro chooses an action $a \\in \\mathcal{A}$ (where $a = (a_1, \\dots, a_N)$ corresponds to actions for every single player) and distributes the correponding actions to the players, which they dutifully play as instructed. This approach can work, but it has a number of drawbacks:\n",
    "- The actions space of the maestro can grows very quickly, if for example each player's action space has fixed size $A$, then the action space of the maestro has size $A^N$, which is exponential in the number of players. If each agent used an independent learning algorithm, each would have to manage only $A$ actions.\n",
    "- In certain context centralized learning is not allowed, for example with autonomous driving or when the details of each agent are not known.\n",
    "- Each player might have a different reward function (i.e. a different objective) and the maestro should take this into account in order to devise a complex learning objective suited to the underlying players. In practice the reward of each player might not be known and even if they are known, it might be non-trivial or downright impossible to aggregate the given rewards into a single one that can be fed to the maestro's learning procedure to achieve the desired solution concept, more on this in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent learning\n",
    "The opposite approach is for each agent to learn using an independent learning procedure, removing the need for a centralized authority, its dependency on the number of players and the issue of handling the rewards. This approach has proven to be much more successfull, but classical algorithm from RL might not be suited to the task.\n",
    "\n",
    "#### Independent Q-learning\n",
    "Consider the case in which each agents learns using the standard Q-learning algorithm. On each round, each player $i \\in \\{1, \\dots, N\\}$ observes a state $s \\in \\mathcal{S}$ and picks an action $a_i \\in \\mathcal{A}_i$, after each player picked their action $a_1, \\dots, a_N$, the next state $s' = \\tau(s, a)$ transitions according the joint action $a = (a_1, \\dots, a_N)$ and each player receives reward $r_i$, which is used to update their respective value function $Q_i$ as\n",
    "$$\n",
    "\tQ_i(s, a_i) =\n",
    "\t\t(1 - \\alpha) Q_i(s, a_i)\n",
    "\t\t+ \\alpha ( r_i + \\gamma \\max_{a'_i \\in \\mathcal{A}_i} Q_i(s', a'_i) )\n",
    "$$\n",
    "Because the transition from one state to another depends on the joint action $a = (a_1, \\dots, a_N)$ and not on a single action, from a single player's point of view the transition function of the MDP depends on the other players actions, because the other players actively learn and are therefore non-stationary in their choice of action, the transition function is non-stationary. In general this means that independent learning does not converge, but in certains classes of games, it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of multi-agent reinforcement learning\n",
    "In the classic RL case, a policy is optimal if it maximises the _return_, defined as the sum of the immediate reward obtained during play, in the multi-agent case, any player's return depends on the other players' actions and ultimately depends on their policies.\n",
    "\n",
    "**If the return of a policy depends on the other policies, how do we define optimality?** There is no fixed answer to this question, in fact it depends on the underlying context and the objective of the learning procedure, for now we only focus on modeling the problem. To understand the different solutions concepts for MARL we will rely on game-theorical arguments.\n",
    "\n",
    "In the meantime, we can evaluate the performance of a learning algorithm by its convergence rate to a specific solution concept, viz. we define a specific policy for each player to which we want our algorithm to converge to and we try to understand\n",
    "1. If the learning algorithm converges to the proposed solution (which the algorithm does not know).\n",
    "2. If true, how fast is the convergence?\n",
    "\n",
    "For example, going back to the football example, before the start of the game we decide _roughly_ how we want the players to play by defining different roles, which translate to different behaviours. Afterward we pick a learning algorithm and try to understand if the players actually learn to play as we intended.\n",
    "\n",
    "This might seem very accidental, how can we be sure that the algorithm will converge to these behaviours that we randomly picked? This can be achieved by choosing these behaviours wisely from very well studied solution concepts, which we will tackle in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the PettingZoo\n",
    "The Farama foundation, who maintains the Gymnasium library, work also on a MARL-oriented library called [PettingZoo](https://pettingzoo.farama.org/). They share the same interface, which is very useful when, hypothetically, you have been following a course using Gym and now you need to abruptly switch library to cover MARL topics.\n",
    "\n",
    "Now a couple of examples: first the simple competitive tic-tac-toe game and then a collaborative combat game with players with different actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize(render):\n",
    "\tclear_output(wait=True)\n",
    "\tplt.imshow(render)\n",
    "\tplt.axis(\"off\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode = \"rgb_array\")\n",
    "env.reset(seed = 42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "\tobs, reward, termination, truncation, info = env.last()\n",
    "\tif termination or truncation:\n",
    "\t\taction = None\n",
    "\telse:\n",
    "\t\tlegal_moves = obs[\"action_mask\"]\n",
    "\t\taction = env.action_space(agent).sample(legal_moves)\n",
    "\n",
    "\tvisualize(env.render())\n",
    "\tenv.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tabular import QLearning\n",
    "\n",
    "# We need two encoding-decoding functions to get from the original action (or state)\n",
    "# space to an action (or state) space which is hashable by Python dictionaries.\n",
    "\n",
    "def encode(xs: np.ndarray) -> tuple[int]:\n",
    "\treturn tuple(xs.flatten())\n",
    "\n",
    "def decode(xs: tuple[int], size = (1, )) -> np.ndarray:\n",
    "\tif len(xs) == 1: return xs[0]\n",
    "\treturn np.array(xs).reshape(size)\n",
    "\n",
    "# Now redefine the QLearning class to use these functions and also to pick the actions\n",
    "# from the specific agent's action space, this last change is foundamental when working\n",
    "# with MARL.\n",
    "\n",
    "class MultiAgentQLearning(QLearning):\n",
    "\tdef greedy(self, state, agent):\n",
    "\t\tdefault_action = encode(self.mdp.action_space(agent).sample())\n",
    "\t\taction = max(self.Q[state], key=self.Q[state].get, default=default_action)\n",
    "\t\treturn decode(action)\n",
    "\n",
    "\tdef explore(self, state, agent):\n",
    "\t\tif np.random.uniform() < self.epsilon:\n",
    "\t\t\treturn self.mdp.action_space(agent).sample()\n",
    "\t\telse:\n",
    "\t\t\treturn self.greedy(state, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_push_v3\n",
    "from tqdm import trange\n",
    "\n",
    "EPISODES = 500\n",
    "MAX_ITERATIONS = 500\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "# env = pistonball_v6.parallel_env(\n",
    "# \tn_pistons = 5,\n",
    "# \tmax_cycles = MAX_ITERATIONS, # after these many cycles the simulation stops\n",
    "# \tcontinuous = False, # the pistons can move down (0), stay still (1) or move up (2)\n",
    "# \trandom_drop = True, # ball appears randomly on the field\n",
    "# \trender_mode = \"rgb_array\"\n",
    "# )\n",
    "\n",
    "env = simple_push_v3.parallel_env(\n",
    "\tmax_cycles = MAX_ITERATIONS,\n",
    "\tcontinuous_actions = False,\n",
    "\trender_mode = \"rgb_array\"\n",
    ")\n",
    "\n",
    "algorithms = {\n",
    "\t\"agent_0\": MultiAgentQLearning(env),\n",
    "\t\"adversary_0\": MultiAgentQLearning(env, final_epsilon=1), # random\n",
    "\t# for agent in env.possible_agents\n",
    "}\n",
    "\n",
    "# Set the epsilon decay value\n",
    "for agent in algorithms.values():\n",
    "\tagent.e_decay = agent.epsilon / (MAX_ITERATIONS / 2)\n",
    "\n",
    "for episode in trange(EPISODES):\n",
    "\tobservations, _ = env.reset()\n",
    "\n",
    "\tobservations_encoded = {\n",
    "\t\tagent: encode(observations[agent])\n",
    "\t\tfor agent in env.possible_agents\n",
    "\t}\n",
    "\n",
    "\t# Keep track of the average error for each agent\n",
    "\tepisode_error = {agent: [] for agent in env.possible_agents}\n",
    "\n",
    "\twhile env.agents:\n",
    "\t\tactions = {\n",
    "\t\t\tagent: algorithms[agent].explore(observations_encoded[agent], agent)\n",
    "\t\t\tfor agent in env.agents\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# visualize(env.render())\n",
    "\t\tnew_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "\t\tnew_observations_encoded = {\n",
    "\t\t\tagent: encode(new_observations[agent])\n",
    "\t\t\tfor agent in env.possible_agents\n",
    "\t\t}\n",
    "\n",
    "\t\tfor agent, algorithm in algorithms.items():\n",
    "\t\t\t# Encode the states and action\n",
    "\t\t\terror = algorithm.update(\n",
    "\t\t\t\tstate = observations_encoded[agent],\n",
    "\t\t\t\taction = encode(actions[agent]),\n",
    "\t\t\t\treward = rewards[agent],\n",
    "\t\t\t\tterminated = terminations[agent],\n",
    "\t\t\t\ts_prime = new_observations_encoded[agent]\n",
    "\t\t\t)\n",
    "\t\t\tepisode_error[agent].append(error)\n",
    "\n",
    "\t\tobservations_encoded = new_observations_encoded\n",
    "\n",
    "\tif episode % SAVE_EVERY == 0:\n",
    "\t\tfor agent, algorithm in algorithms.items():\n",
    "\t\t\t# algorithm.history.append(algorithm.Q.copy())\n",
    "\t\t\talgorithm.error.append(np.average(episode_error[agent]))\n",
    "\n",
    "\tfor algorithm in algorithms.values():\n",
    "\t\talgorithm.decay_epsilon()\n",
    "\t\t\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(1, 1 + EPISODES // SAVE_EVERY)\n",
    "\n",
    "for agent, algorithm in algorithms.items():\n",
    "\tplt.plot(X, algorithm.error, label=agent)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = simple_push_v3.parallel_env(\n",
    "\tmax_cycles = 100,\n",
    "\tcontinuous_actions = False,\n",
    "\trender_mode = \"rgb_array\"\n",
    ")\n",
    "\n",
    "observations, _ = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "\tactions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "\tvisualize(env.render())\n",
    "\tobservations, rewards, terminations, truncations, _ = env.step(actions)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
