{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-agent reinforcement learning\n",
    "\n",
    "<small>see sections 1.1 to 1.4 and 5.3 to 5.4 of [Multi-Agent Reinforcement Learning: Foundations and Modern Approaches](https://www.marl-book.com) for a more in-depth overview.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we tackled the problem of learning an optimal policy for a single agent inside a MDP, we now shift our focus towards contexts in which there are _multiple_ agents acting in the MDP with (possibly) different goals.\n",
    "\n",
    "A **multi-agent system** is composed of an environment and multiple decision-making agents, also called _players_, that interact in the environment to achieve certain goals. The environment evolves during play and the players refine their respective strategies, which can be collaborative, competitive or a mix of the two. **multi-agent reinforcement learning** or **MARL** tackles learning in such systems.\n",
    "\n",
    "As an example, consider a fleet of autonomous vehicles with the goal of driving as quickly and safely as possible, a team of competing players in a videogame or a group of automated traders that manage each their own investments in a public market. See the [environments](https://pettingzoo.farama.org/environments/classic/) from the PettinZoo library for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of a round of the game, each player sees the state of the environment and picks an action, the round ends after every player has chosen an action, then the envoriment evolves accordingly and each players collects their reward to update their respective policy.\n",
    "\n",
    "From the point of view of the agent little has changed, they still have a set of actions, a policy, an enviroment and the goal of optimizing their policy. The difference lies in the fact that **the immediate reward achieved by any player depends on the actions of the other players on the same round**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of multi-agent reinforcement learning\n",
    "In the classic RL case, a policy is optimal if it maximises the _return_, defined as the sum of the immediate reward obtained during play, in the multi-agent case, any player's return depends on the other players' actions and ultimately depends on their policies.\n",
    "\n",
    "**If the return of a policy depends on the other policies, how do we define optimality?** There is no fixed answer to this question, in fact it depends on the underlying context and the objective of the learning procedure, for now we only focus on modeling the problem. To understand the different solutions concepts for MARL we will rely on game-theorical arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the PettingZoo\n",
    "The Farama foundation, who maintains the Gymnasium library, work also on a MARL-oriented library called [PettingZoo](https://pettingzoo.farama.org/). They share the same interface, which is very useful when, hypothetically, you have been following a course using Gym and now you need to abruptly switch library to cover MARL topics.\n",
    "\n",
    "Now a couple of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = tictactoe_v3.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "\tobservation, reward, termination, truncation, info = env.last()\n",
    "\tif termination or truncation:\n",
    "\t\taction = None\n",
    "\telse:\n",
    "\t\t# Sample only legal moves\n",
    "\t\tlegal_moves = observation[\"action_mask\"]\n",
    "\t\taction = env.action_space(agent).sample(legal_moves)\n",
    "\tenv.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "env = knights_archers_zombies_v10.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter(max_iter = 100):\n",
    "\tobservation, reward, termination, truncation, info = env.last()\n",
    "\taction = env.action_space(agent).sample()\n",
    "\tenv.step(action)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
