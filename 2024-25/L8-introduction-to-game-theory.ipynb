{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Universit√† degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# Introduction to game theory\n",
    "\n",
    "### Luigi Foscari\n",
    "\n",
    "<small>see sections 3.1, 3.3, 4.1, 4.2, 4.4 and 4.7 of [Multi-Agent Reinforcement Learning: Foundations and Modern Approaches](https://www.marl-book.com) for a more in-depth overview.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we discovered why switching from single agent to multiple agents can be challenging. In this lesson we try to model multi-agent interaction with a theoretical foundation. We start with **normal form games** which describe _one-shot_ interactions between multiple agents with no evolving environment and provide the necessary foundations to understand the objective and problems, then we switch to **stochastic games** that model multiple interactions between a group of agents in an evolving environment.\n",
    "\n",
    "<img src=\"imgs/game-hierarchy.png\" width=\"600\" style=\"display: block; margin: 0 auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes\n",
    "\n",
    "First recall that a Markov decision process is a tuple composed of\n",
    "- Finite set of states $\\mathcal{S}$\n",
    "- Finite set of actions $\\mathcal{A}$\n",
    "- Reward function $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$\n",
    "- State transition probability function $\\{ p(\\cdot \\mid s, a) \\mid s \\in \\mathcal{S}, a \\in \\mathcal{A} \\}$ defined on a transition kernel $p(\\cdot \\mid s, a)$ over $\\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal form games\n",
    "\n",
    "A normal form (or _strategic form_) game is a model that describes the interaction of multiple agents in a static environment. A normal form game is defined as a tuple composed of\n",
    "- Set of $N$ agents $I = \\{1, \\dots, N\\}$\n",
    "- For each agent $i \\in I$:\n",
    "\t- Finite set of actions $\\mathcal{A}_i$\n",
    "\t- Reward function $r_i: \\mathcal{A} \\to \\mathbb{R}$ where $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_n$\n",
    "\n",
    "As expected, there are no states or a transition function. This form is called _normal_ in constract to the _extensive_ form, which is more involved and can describe sequential moves, chance moves and even partial information, we won't study extensive form games.\n",
    "\n",
    "Each player of a normal form game has a policy or strategy $\\pi_i$ used to describe their action, this policy defines a probability distribution over the actions, such that player $i$ plays action $a_i$ with probability $\\pi_i(a_i)$. Each player simoultaneously chooses an action according to their probability distribution and the chosen actions together form the _joint action_ $a = (a_1, a_2, \\dots, a_N)$. The reward given to each player depends on the joint action and the respective reward function $r_i(a)$, not on any player's action alone.\n",
    "\n",
    "We call a strategy **pure** if it plays one action with probability 1 and the others with probability 0, this is the constant strategy. A strategy strategy with positive probabilities on multiple actions is called **mixed**.\n",
    "\n",
    "Types of normal-form games include\n",
    "- **zero-sum games**: a normal form game is called _zero-sum_ if the sum of the rewards perceived by the players is always zero, for example for two players $r_1(a) = -r_2(a)$ for any joint action $a \\in \\mathcal{A}$. A classic example is rock-paper-scissors.\n",
    "- **common-reward games**: a normal form game is called _common-reward_ if the same reward is given to every player $r_1(a) = r_2(a) = \\dots = r_N(a)$. Also called *cooperation games*, a typical example has to do with modelling swarm behaviours.\n",
    "- **general-sum games**: a normal form game is called _general-sum_ if no restrictions are placed on the reward function. For example the prisoner's dilemma.\n",
    "\n",
    "We focus on two-player general-sum games. We represent these games as matrices where each row is the reward of one player and each column the reward of the other, each cell is a joint action. Note that the only randomness lies in the players' policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prisoner's dilemma\n",
    "\n",
    "<img src=\"imgs/prisoner-dilemma.jpg\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "The prisoner's dilemma is a classic example of general-sum normal form game. Andrea and Bryan are two criminals that get caught and put in separate cells before interrogation. Fortunately for them, there is not enough evicence for a full conviction. After questioning, the officer separately tells them that they can either **collaborate** with the police by confessing to the crime or remain **silent**. If only one confesses, they get out of prison, but the other is sentenced to 10 years in jail. If both confess, they get 8 years each. If both stay silent, the police has makes a minor conviction of 1 year each.\n",
    "\n",
    "We can write the game in matrix form as\n",
    "<center>\n",
    "\n",
    "|                 | **Stay silent** | **Confess** |\n",
    "|-----------------|-----------------|-------------|\n",
    "| **Stay silent** |    $(-1, -1)$   |  $(-10, 0)$ |\n",
    "| **Confess**     |    $(0, -10)$   |  $(-8, -8)$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "The prisoners make their decision separately, what should they do? If they both stay silent they get only one year each, knowing that, one prisoner might try to predict that the other will remain silent and confess, but if the other does the same, they both confess.\n",
    "\n",
    "<small>This is also true if the game is repeated a fixed number of times between the same two players and they want to maximise their respective cumulative reward. Try to work out why. Hint: start from the last round.</small>\n",
    "\n",
    "Suppose that one prisoner knows what the other is going to do, if the other is going to stay silent, the best move is to confess, instead if the other is going to confess, the best move is to confess as well. In the end the best play seems to be to confess regardless.\n",
    "\n",
    "Formally, the strategy $\\pi_i$ which picks action $a_i = $ _confess_ with probability 1 is **dominant**, meaning that achieves better reward to any other policy regardless of what the other player will do.\n",
    "\n",
    "This game was studied under the lens of philosophy and some provided a different approach with leads to the opposite conclusion. From _\"The prisoner's dilemma paradox: rationality, morality, and reciprocity\"_ by Rory W. Collins:\n",
    "\n",
    "> Several philosophers have advanced a case for staying silent on rational grounds called the symmetry argument [Bicchieri and Green (1997)]. The general idea is that Andrea knows she and Bryan are both rational agents, so whatever choice she arrives at, Bryan will also reach, since two rational agents will reason symmetrically. This reduces the four possibilities to just two: both confess or both stay silent. Of these, the latter is clearly preferable, so Andrea should remain silent, confident that Bryan will too.\n",
    "\n",
    "Fun fact: [the prisoner's dilemma is also a Magic: the Gathering card](https://scryfall.com/card/mkc/34/prisoners-dilemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game of chicken\n",
    "\n",
    "<img src=\"imgs/game-of-chicken.jpg\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "Similarly to the prisoner's dilemma, the game of chicken describes a situation in which the optimal move is not obvious and depends entirely on the opponent's approach. This time two drivers, here depicted are two chickens on bicycles, are speeding towards each other, if neither swerves they will crash, if only one swerves, the other will win the game of chicken and the glory, if both swerve, none will win, but they will both get home safely to their chicks.\n",
    "\n",
    "It is difficult to represent the danger of crashing into each other, call the extent of the damage $M$ and define the game as\n",
    "<center>\n",
    "\n",
    "|            | **Swerve** | **Drive**  |\n",
    "|------------|------------|------------|\n",
    "| **Swerve** | $(1, 1)$   | $(-1, 2)$  |\n",
    "| **Drive**  | $(2, -1)$  | $(-M, -M)$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "As before, assume that you are playing this game and know that the other player will drive, in that case you better swerve, otherwise you better drive. This time there is not dominant strategy which picks always the same action (also known as a **pure** strategy). We instead have to look for a **mixed** strategy, one that picks one action with a certain probability $p \\in [0, 1]$ (when $p = 0$ or $p = 1$ the strategy is pure, in this sense every pure strategy is a mixed strategy as well) and the other with probability $1 - p$.\n",
    "\n",
    "We want to maximise the reward under any strategy the opponent might use, call $q$ the probability that the opponent will swerve and $1 - q$ the probability that they will keep driving. Therefore, if you swerve, you will receive expected reward $q \\cdot 1 + (1 - q) \\cdot (-1) = 2q - 1$ and if you keep driving expected reward $q \\cdot 2 + (1 - q) \\cdot (-M) = (2 + M)q - M$. For our strategy to be the best, we need to get the same reward from both actions, otherwise we could just always pick one and we'd be done, so we want\n",
    "$$\n",
    "\t2q - 1 = (2 + M)q - M\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "\tq = 1 - \\frac{1}{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions concepts and Nash equilibrium\n",
    "So far we analyzed specific games, now we take a step back and look for a more general approach. When inspecting a game, we want to understand how rational players will play this game.\n",
    "\n",
    "Consider the prisoner's dilemma and a player who only confesses, the other player's best strategy in this case is simply to always confess, leading to the highest payoff. Formally, a rational players would never play a **strictly dominated** strategy, i.e. a strategy for which exists another strategy leading to a better payoff for every action.\n",
    "\n",
    "This is a simple procedure to find dominant pure strategies. Consider a set containing all the pure strategies available to a single player, then remove a strategy if it is dominated by at least another action in the set (i.e. if there is another pure strategy which yields strictly better payoff for every action chosen by the adversary). Repeat this process until you are left with a set of non dominated strategy, if what is left is a single strategy, then the game is **solved** because you found a **strictly dominant strategy**. If you are left with multiple strategies, which should you pick?\n",
    "\n",
    "If we assume that every player is rational and that this fact is common knowledge among the players, what strategy will the players pick? The best option would be for both to pick a strictly dominant strategy. Unfortunately, a pair of strictly dominant strategies might not always exist, regardless of whether they are pure.\n",
    "\n",
    "Focusing on the Prisoner's dilemma, consider the pair of strategies where both players stay silent and both get payoff $-1$. If one player instead confessed, they would the $0$, in this case the strategy which always stays silent is strictly dominated by the strategy which always confesses. The original pair of strategies is _unstable_, in the sense that at least one player would change their strategy is favour of a dominant one, we want to find pairs of strategies which are _stable_, which can be defined as:\n",
    "\n",
    "> A pair of strategies is **stable** if no player would gain by switching to another strategy unilaterally.\n",
    "\n",
    "Turns out that these strategy pairs have a name and are called **Nash equilibria** and a very famous theorem by Nash (hence the name) reassures us of the following\n",
    "> **Nash's Theorem**:\n",
    "> For any general-sum finite game with at least 2 players, there exists at least one (possibly mixed) Nash equilibrium.\n",
    "\n",
    "Furthermore, a game might also admit multiple Nash equilibria, yielding different payoffs to the players. We can look for a Nash equilibrium in the Prisoner's dilemma, we know that the strategy pair in which both stay silent is not stable, but if the strategy pair in which both players confess is stable.\n",
    "\n",
    "Note that this is not a surprise, because the Prisoner's dilemma admits a single strictly dominant strategy and because the players are rational, this strategy has to be played in the equilibrium.\n",
    "\n",
    "Can we conclude that the pair of strategies in which both players confess is the best possible outcome? ABSOLUTELY NOT, because it does not yield the best possible payoff!\n",
    "\n",
    "Try for yourself:\n",
    "- Can you find the dominant strategies for the Chicken game?\n",
    "- Can you find a Nash equilibrium for the Chicken game?\n",
    "<br><small>Hint: there are multiple.</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing equilibria\n",
    "After some consideration, is not too hard to find equilibria in either the chicken game of the prisoner's dilemma, but can we find a general procedure? We know that a at least one Nash equilibrium must exist, can we find it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic games\n",
    "We now introduce the notation necessary to formalize the proposed multi-agent systems. Little has changed from the MDPs in RL, we simply need to add multiple players and tweak the reward function.\n",
    "\n",
    "Consider a multi-agent system with $N > 2$ players identified by their index, call $I = \\{ 1, \\dots, N \\}$ the set of players. The actions $a = (a_1, \\dots, a_N)$ are composed of $N$ player actions, define a set $\\mathcal{A}_i$ for each player $i \\in I$ and call $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_N$ the action set. The reward function $r$ is defined on this new action set and we assume that it can change from player to player (which translates into players with different objectives).\n",
    "\n",
    "Finally we define this model as a tuple of\n",
    "- Finite set of players $I = \\{ 1, \\dots, N \\}$\n",
    "- Finite set of states $\\mathcal{S}$\n",
    "- For each player $i \\in I$:\n",
    "\t- Finite set of actions $\\mathcal{A}_i$\n",
    "\t- Reward function $r_i: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ where $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_N$\n",
    "- State transition probability function $\\{ p(\\cdot \\mid s, a) \\mid s \\in \\mathcal{S}, a \\in \\mathcal{A} \\}$ defined on a transition kernel $p(\\cdot \\mid s, a)$ over $\\mathcal{S}$.\n",
    "\n",
    "This model takes the name of **Stochastic game**, Shapley game, or even Markov game (even though Markov had nothing to do with it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
