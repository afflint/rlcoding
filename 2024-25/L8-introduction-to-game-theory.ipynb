{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Università degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# Introduction to game theory\n",
    "\n",
    "### Luigi Foscari\n",
    "\n",
    "<small>see sections 3.1, 3.3, 4.1, 4.2, 4.4 and 4.7 of [Multi-Agent Reinforcement Learning: Foundations and Modern Approaches](https://www.marl-book.com) for a more in-depth overview.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we discovered why switching from single agent to multiple agents can be challenging. In this lesson we try to model multi-agent interaction with a theoretical foundation. We start with **normal form games** which describe _one-shot_ interactions between multiple agents with no evolving environment and provide the necessary foundations to understand the objective and problems, then we switch to **stochastic games** that model multiple interactions between a group of agents in an evolving environment.\n",
    "\n",
    "<img src=\"imgs/game-hierarchy.png\" width=\"600\" style=\"display: block; margin: 0 auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes\n",
    "\n",
    "First recall that a Markov decision process is a tuple composed of\n",
    "- Finite set of states $\\mathcal{S}$\n",
    "- Finite set of actions $\\mathcal{A}$\n",
    "- Reward function $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$\n",
    "- State transition probability function $\\{ p(\\cdot \\mid s, a) \\mid s \\in \\mathcal{S}, a \\in \\mathcal{A} \\}$ defined on a transition kernel $p(\\cdot \\mid s, a)$ over $\\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal form games\n",
    "\n",
    "A normal form (or _strategic form_) game is a model that describes the interaction of multiple agents in a static environment. A normal form game is defined as a tuple composed of\n",
    "- Set of $N$ agents $I = \\{1, \\dots, N\\}$\n",
    "- For each agent $i \\in I$:\n",
    "\t- Finite set of actions $\\mathcal{A}_i$\n",
    "\t- Reward function $r_i: \\mathcal{A} \\to \\mathbb{R}$ where $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_n$\n",
    "\n",
    "As expected, there are no states or a transition function. This form is called _normal_ in constract to the _extensive_ form, which is more involved and can describe sequential moves, chance moves and even partial information, we won't study extensive form games.\n",
    "\n",
    "Each player of a normal form game has a policy or strategy $\\pi_i$ used to describe their action, this policy defines a probability distribution over the actions, such that player $i$ plays action $a_i$ with probability $\\pi_i(a_i)$. Each player simoultaneously chooses an action according to their probability distribution and the chosen actions together form the _joint action_ $a = (a_1, a_2, \\dots, a_N)$. The reward given to each player depends on the joint action and the respective reward function $r_i(a)$, not on any player's action alone.\n",
    "\n",
    "We call a strategy **pure** if it plays one action with probability 1 and the others with probability 0, this is the constant strategy. A strategy strategy with positive probabilities on multiple actions is called **mixed**.\n",
    "\n",
    "Types of normal-form games include\n",
    "- **zero-sum games**: a normal form game is called _zero-sum_ if the sum of the rewards perceived by the players is always zero, for example for two players $r_1(a) = -r_2(a)$ for any joint action $a \\in \\mathcal{A}$. A classic example is rock-paper-scissors.\n",
    "- **common-reward games**: a normal form game is called _common-reward_ if the same reward is given to every player $r_1(a) = r_2(a) = \\dots = r_N(a)$. Also called *cooperation games*, a typical example has to do with modelling swarm behaviours.\n",
    "- **general-sum games**: a normal form game is called _general-sum_ if no restrictions are placed on the reward function. For example the prisoner's dilemma.\n",
    "\n",
    "We focus on two-player general-sum games. We represent these games as matrices where each row is the reward of one player and each column the reward of the other, each cell is a joint action. Note that the only randomness lies in the players' policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prisoner's dilemma\n",
    "\n",
    "<img src=\"imgs/prisoner-dilemma.jpg\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "The prisoner's dilemma is a classic example of general-sum normal form game. Andrea and Bryan are two criminals that get caught and put in separate cells before interrogation. Fortunately for them, there is not enough evicence for a full conviction. After questioning, the officer separately tells them that they can either **collaborate** with the police by confessing to the crime or remain **silent**. If only one confesses, they get out of prison, but the other is sentenced to 10 years in jail. If both confess, they get 8 years each. If both stay silent, the police has makes a minor conviction of 1 year each.\n",
    "\n",
    "We can write the game in matrix form as\n",
    "<center>\n",
    "\n",
    "|                 | **Stay silent** | **Confess** |\n",
    "|-----------------|-----------------|-------------|\n",
    "| **Stay silent** |    $(-1, -1)$   |  $(-10, 0)$ |\n",
    "| **Confess**     |    $(0, -10)$   |  $(-8, -8)$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "<small>Note that the exact numbers in the matrix are of little importance, other variations might have different numbers, but the logic remains the same.</small>\n",
    "\n",
    "The prisoners make their decision separately, what should they do? If they both stay silent they get only one year each, knowing that, one prisoner might try to predict that the other will remain silent and confess, but if the other does the same, they both confess.\n",
    "\n",
    "<small>This is also true if the game is repeated a fixed number of times between the same two players and they want to maximise their respective cumulative reward. Try to work out why. Hint: start from the last round.</small>\n",
    "\n",
    "Suppose that one prisoner knows what the other is going to do, if the other is going to stay silent, the best move is to confess, instead if the other is going to confess, the best move is to confess as well. In the end the best play seems to be to confess regardless.\n",
    "\n",
    "Formally, the strategy $\\pi_i$ which picks action $a_i = $ _confess_ with probability 1 is **dominant**, meaning that achieves better reward to any other policy regardless of what the other player will do.\n",
    "\n",
    "This game was studied under the lens of philosophy and some provided a different approach with leads to the opposite conclusion. From _\"The prisoner's dilemma paradox: rationality, morality, and reciprocity\"_ by Rory W. Collins:\n",
    "\n",
    "> Several philosophers have advanced a case for staying silent on rational grounds called the symmetry argument [Bicchieri and Green (1997)]. The general idea is that Andrea knows she and Bryan are both rational agents, so whatever choice she arrives at, Bryan will also reach, since two rational agents will reason symmetrically. This reduces the four possibilities to just two: both confess or both stay silent. Of these, the latter is clearly preferable, so Andrea should remain silent, confident that Bryan will too.\n",
    "\n",
    "Fun fact: [the prisoner's dilemma is also a Magic: the Gathering card](https://scryfall.com/card/mkc/34/prisoners-dilemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game of chicken\n",
    "\n",
    "<img src=\"imgs/game-of-chicken.jpg\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "\n",
    "Similarly to the prisoner's dilemma, the game of chicken describes a situation in which the optimal move is not obvious and depends entirely on the opponent's approach. This time two drivers, here depicted are two chickens on bicycles, are speeding towards each other, if neither swerves they will crash, if only one swerves, the other will win the game of chicken and the glory, if both swerve, none will win, but they will both get home safely to their chicks.\n",
    "\n",
    "It is difficult to represent the danger of crashing into each other, call the extent of the damage $M$ and define the game as\n",
    "<center>\n",
    "\n",
    "|            | **Swerve** | **Drive**  |\n",
    "|------------|------------|------------|\n",
    "| **Swerve** | $(1, 1)$   | $(-1, 2)$  |\n",
    "| **Drive**  | $(2, -1)$  | $(-M, -M)$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "As before, assume that you are playing this game and know that the other player will drive, in that case you better swerve, otherwise you better drive. This time there is not dominant strategy which picks always the same action (also known as a **pure** strategy). We instead have to look for a **mixed** strategy, one that picks one action with a certain probability $p \\in [0, 1]$ (when $p = 0$ or $p = 1$ the strategy is pure, in this sense every pure strategy is a mixed strategy as well) and the other with probability $1 - p$.\n",
    "\n",
    "We want to maximise the reward under any strategy the opponent might use, call $q$ the probability that the opponent will swerve and $1 - q$ the probability that they will keep driving. Therefore, if you swerve, you will receive expected reward $q \\cdot 1 + (1 - q) \\cdot (-1) = 2q - 1$ and if you keep driving expected reward $q \\cdot 2 + (1 - q) \\cdot (-M) = (2 + M)q - M$. For our strategy to be the best, we need to get the same reward from both actions, otherwise we could just always pick one and we'd be done, so we want\n",
    "$$\n",
    "\t2q - 1 = (2 + M)q - M\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "\tq = 1 - \\frac{1}{M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions concepts and Nash equilibrium\n",
    "When inspecting a game, we want to understand how rational players will play this game. Consider the prisoner's dilemma and a player who only confesses, the other player's best strategy in this case is to confess as well, leading to the highest payoff. Formally, a rational players would never play a **strictly dominated** strategy, i.e. a strategy for which exists another strategy leading to a better payoff for every action of the adversary.\n",
    "\n",
    "This is a simple procedure to find dominant pure strategies. Consider a set containing all the pure strategies available to a single player, then remove a strategy if it is dominated by at least another action in the set. Repeat this process until you are left with a set of non dominated strategies, if what is left is a single strategy, then the game is **solved** because you found a **strictly dominant strategy**. If you are left with multiple strategies, which should you pick?\n",
    "\n",
    "If we assume that every player is rational and that this fact is common knowledge among the players, then they would pick a strictly dominant strategy, but there might not be in the game a pair of pure _strictly_ dominant strategies, instead each player can look at their set of pure dominant strategies and either pick one and follow it or consider defining a probability distribution over them and use the resulting mixed strategy, which is also dominant. Begin rational, they have no indentive in using dominated strategies.\n",
    "\n",
    "Focusing on the Prisoner's dilemma, consider the pair of strategies where both players stay silent and both get payoff $-1$. If one player instead confessed, they would the $0$, in this case the strategy which always stays silent is strictly dominated by the strategy which always confesses. The original pair of strategies is _unstable_, in the sense that at least one player would change their strategy is favour of a dominant one, we want to find pairs of strategies which are _stable_, which can be defined as:\n",
    "\n",
    "> **Definition**\n",
    "> A pair of strategies is **stable** if no player would gain by switching to another strategy unilaterally.\n",
    "\n",
    "Thus, a natual objective is to find a pair of dominant strategies which is also stable (note that any stable strategy pair is made up of dominant strategies, otherwise either player would gain by switching do a dominant strategy). Turns out that these strategy pairs have a name and are called **Nash equilibria**, a very famous theorem by Nash (hence the name) reassures us of the following\n",
    "> **Nash's Theorem**\n",
    "> For any general-sum finite game with at least 2 players, there exists at least one (possibly mixed) Nash equilibrium.\n",
    "\n",
    "Furthermore, a game might also admit multiple Nash equilibria, yielding different payoffs to the players. We can look for a Nash equilibrium in the Prisoner's dilemma, we know that the strategy pair in which both stay silent is not stable, but if the strategy pair in which both players confess is stable.\n",
    "\n",
    "Note that this is not a surprise, because the Prisoner's dilemma admits a single strictly dominant strategy and because the players are rational, this strategy has to be played in the equilibrium.\n",
    "\n",
    "Can we conclude that the pair of strategies in which both players confess is the best possible outcome? ABSOLUTELY NOT, because it does not yield the best possible payoff!\n",
    "\n",
    "Try for yourself:\n",
    "- Can you find the dominant strategies for the Chicken game?\n",
    "- Can you find a Nash equilibrium for the Chicken game?\n",
    "<br><small>Hint: there are multiple.</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing equilibria\n",
    "After some consideration, is not too hard to find equilibria in either the chicken game of the prisoner's dilemma, but can we find a general procedure? We know that a at least one Nash equilibrium must exist, can we find it?\n",
    "\n",
    "Consider only the case in which there are two players:\n",
    "- For zero sum games, a Nash equilibrium can be efficiently computed using linear programming, where problem that gives the strategy for the column player is the dual of the problem that gives the strategy for the row player. See [Algorithmic game theory, §1.4.2](https://www.cambridge.org/core/books/algorithmic-game-theory/0092C07CA8B724E1B1BE2238DDD66B38) for more information.\n",
    "- For general sum games, this problem becomes very hard, in fact it becomes exponential in the number of total actions in the game. To understand why, see [Algorithmic game theory, §2.1.1](https://www.cambridge.org/core/books/algorithmic-game-theory/0092C07CA8B724E1B1BE2238DDD66B38).\n",
    "\n",
    "What about cooperative games? Can you define a notion of Nash equilibrium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks on Nash equilibria\n",
    "\n",
    "Nash equilibria are a quite natural concept that arises when dicussing games, but unfortunately they are quite hard to compute in general, is there something more that we can say about a game? There are **many** notions of equilibrium and Nash equilibrium is one of the toughest to deal with, there are instead larger classes which contain more general notions that are easier to compute, like correlated equilibria, coarse correlated equilibria or generalized Nash equilibria. Furthermore equilibria are interesting to study also for how they transfers over to other types of game, like sequential games (in which the players pick their actions in order, like chess), games where the players don't know exactly the rules of the game or games where the payoff is probabilitic.\n",
    "\n",
    "See the [Algorithmic game theory](https://www.cambridge.org/core/books/algorithmic-game-theory/0092C07CA8B724E1B1BE2238DDD66B38), [Game theory, alive](https://bookstore.ams.org/mbk-101/) and [Game theory](https://www.cambridge.org/core/product/identifier/9780511794216/type/book) for more details.\n",
    "\n",
    "One can ask themselves why do we care about computing Nash equilibria, consider the following quote from [Algorithmic game theory, §2.1](https://www.cambridge.org/core/books/algorithmic-game-theory/0092C07CA8B724E1B1BE2238DDD66B38)\n",
    "> We believe that this matter of computational complexity is one of central importance here, and indeed that the algorithmic point of view has much to contribute to the debate of economists about solution concepts. The reason is simple: If an equilibrium concept is not efficiently computable, much of its credibility as a prediction of the behavior of rational agents is lost – after all, there is no clear reason why a group of agents cannot be simulated by a machine. Efficient computability is an important modeling perequisite for solution concepts. In the words of Kamal Jain, “If your laptop cannot find it, neither can the market.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic games\n",
    "We now introduce the notation necessary to formalize the proposed multi-agent systems. Little has changed from the MDPs in RL, we simply need to add multiple players and tweak the reward function.\n",
    "\n",
    "Consider a multi-agent system with $N > 2$ players identified by their index, call $I = \\{ 1, \\dots, N \\}$ the set of players. The actions $a = (a_1, \\dots, a_N)$ are composed of $N$ player actions, define a set $\\mathcal{A}_i$ for each player $i \\in I$ and call $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_N$ the action set. The reward function $r$ is defined on this new action set and we assume that it can change from player to player (which translates into players with different objectives).\n",
    "\n",
    "Finally we define this model as a tuple of\n",
    "- Finite set of players $I = \\{ 1, \\dots, N \\}$\n",
    "- Finite set of states $\\mathcal{S}$\n",
    "- For each player $i \\in I$:\n",
    "\t- Finite set of actions $\\mathcal{A}_i$\n",
    "\t- Reward function $r_i: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ where $\\mathcal{A} := \\mathcal{A}_1 \\times \\dots \\times \\mathcal{A}_N$\n",
    "- State transition probability function $\\{ p(\\cdot \\mid s, a) \\mid s \\in \\mathcal{S}, a \\in \\mathcal{A} \\}$ defined on a transition kernel $p(\\cdot \\mid s, a)$ over $\\mathcal{S}$.\n",
    "\n",
    "This model takes the name of **Stochastic game**, Shapley game, or even Markov game (even though Markov had nothing to do with it).\n",
    "\n",
    "In the single-agent case we had a single policy from states to actions, here we have $N$ policies $\\pi_1, \\dots, \\pi_N$, which together form a policy profile $\\pi = (\\pi_1, \\dots, \\pi_N)$ mapping states $s$ to action profile $a_t = (a_{t, 1}, \\dots, a_{t, N})$, we can compute the discounted reward for any state $s_0$ and any player $i$ under policy profile $\\pi$ as\n",
    "$$\n",
    "\tV_i(\\pi \\mid s_0) = \\mathbb{E}\\left[\n",
    "\t\t\\sum_{t=0}^{\\infty} \\gamma^t r_i(s_t)\n",
    "\t\\right]\n",
    "\t\\qquad\n",
    "\t\\text{where}\n",
    "\t\\quad\n",
    "\ts_{t+1} \\sim p(\\cdot \\mid s_t, a_t)\n",
    "\t\\quad\n",
    "\ta_t = (a_{t, 1}, \\dots, a_{t, N}) \\sim \\pi(s_t)\n",
    "$$\n",
    "\n",
    "#### Nash policies\n",
    "\n",
    "Earlier we discussed Nash _equilibria_, this notion trasfers from the normal form games to the stochastic games with the notion of Nash _policies_, which are strategies from which no player would want to deviate unilaterally, formally consider a game with $N$ players, each with a policy $\\pi_i$ for $i = 1, \\dots N$ forming the policy profile $\\pi := (\\pi_1, \\dots, \\pi_N)$, the discounted reward achieved by any player $i$ under policy profile $\\pi$ on state $s$ is $V^i_s(\\pi)$, we say that $\\pi$ is a Nash policy (profile) if\n",
    "$$\n",
    "V_i(\\pi_i', \\pi_{-i} \\mid s) \\le V^i_s(\\pi \\mid s)\n",
    "\\qquad\n",
    "\\forall \\pi_i'\n",
    "\\quad\n",
    "\\forall i\n",
    "\\quad\n",
    "\\forall s\n",
    "$$\n",
    "where $\\pi_{-i}$ is the policy profile $\\pi$ without the $i$-th policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
