{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Universit√† degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# DRL\n",
    "## Deep Reinforcement Learning Overview\n",
    "\n",
    "### Alfio Ferrara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting Neural Networks in RL\n",
    "\n",
    "Neural Networks are a natural way of approximating state value and action-state value functions but also the policy.\n",
    "\n",
    "![](./imgs/drl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-based methods\n",
    "The idea of policy-based methods is to directly parametrized a policy, either as a **stochastic policy** or as a **deterministic policy**\n",
    "- $\\pi(a \\mid s; \\mathbf{w})$ (stochastic)\n",
    "- $\\pi(s; \\mathbf{w}) \\rightarrow a$ (deterministic)\n",
    "where $\\mathbf{w}$ are the parameters in form of a neural network\n",
    "\n",
    "**Objective function**\n",
    "The goal of policy-based methods is to **maximize** the **expected cumulative reward**. In order to learn the policy, we then exploit **gradient ascent** as for a maximization process. The objective function is thus, given a trajectory $\\tau$ sampled from the policy:\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ R(\\tau)\\right]\n",
    "$$ \n",
    "In case of episodic mdps we have:\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\sum\\limits_{t=1}^{T} r_t \\right]\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are updated accorging to the **policy gradient theorem** that states:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\nabla_{\\mathbf{w}} \\log \\pi(a \\mid s; \\mathbf{w}) Q(s, a)\\right]\n",
    "$$\n",
    "where the way changes in parameters affect the choice of an action is measured by $\\nabla_{\\mathbf{w}} \\log \\pi(a \\mid s; \\mathbf{w})$, while the value of taking action $a$ in state $s$ is measured by $Q(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value-based methods (DQN in particular)\n",
    "\n",
    "With value-based methods, we aim at estimating the $S(s, a; \\mathbf{w})$ function. With **Deep Q-Learning (DQN)** we exploit a neural network in order to **minimize** the loss function using gradient descent and the Q-Learning target:\n",
    "$$\n",
    "Q(s, a; \\mathbf{w}) \\rightarrow Q(s, a; \\mathbf{w}) + \\alpha \\left[ r + \\gamma \\max\\limits_{a'} Q(s', a'; \\mathbf{w}^{-}) - Q(s, a; \\mathbf{w})\\right]\n",
    "$$\n",
    "\n",
    "where the loss function $L{\\mathbf{w}}$ is defined as\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\mathbb{E}\\left[ \\left( r + \\gamma \\max\\limits_{a'} Q(s', a'; \\mathbf{w}^{-}) - Q(s, a; \\mathbf{w}) \\right)^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that** we use a **first set of parameters** for the current estimation $Q(s, a; \\mathbf{w})$ and a **different set of parameters** (i.e., another neural network, called **target network**) for the target $Q(s', a'; \\mathbf{w}^{-})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN introduces some techniques to improve stability (see the details in [DQN](./L4.1-dqn.ipynb))\n",
    "- **Experience replay**: we use a buffer of previous $(s, a, r, s')$ in order to always update on a batch of observations sampled from the past in order to improve stability and break correlation\n",
    "- **Target network**: We use a separate network $\\mathbf{w}^{-}$ to compute the target in order to reduce instability. $\\mathbf{w}^{-} \\leftarrow \\mathbf{w}$ periodically.\n",
    "- **DDQN (Double DQN)**: The value estimation of DQN ($\\max\\limits_{a'}Q(s', s')$) overestimates the values of state action pairs because it is based on the maximum value. DDQN uses the current network for action selection and the target network for value estimation: $r + \\gamma Q(s', \\argmax\\limits_{a'}Q(s', a'; \\mathbf{w}); \\mathbf{w}^{-})$\n",
    "- **Advantage DQN (Dueling DQN)**: we split $Q(s, a)$ estimation in two parts: the value of state $s$ and the relative importance of actions, evluated by an **advantage function** $A(s, a)$:\n",
    "$$\n",
    "Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{\\mid \\mathcal{A} \\mid} \\sum\\limits_{a'} A(s, a')\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
