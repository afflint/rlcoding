{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Universit√† degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# Policy-based methods\n",
    "## An overview of policy gradient methods\n",
    "\n",
    "### Alfio Ferrara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Theorem\n",
    "The intuition on the base of policy gradient is that policy gradient methods transform the policy output into a probability distribution, from which the agent samples to select an action. These methods adjust the policy parameters, continuously refining the probability distribution at each iteration. As a result, the updated distribution increases the likelihood of selecting actions that yield higher rewards.\n",
    "\n",
    "The policy gradient algorithm calculates the gradient of the expected return concerning the policy parameters. By adjusting these parameters in the direction of the gradient, the agent enhances the probability of choosing actions that lead to greater rewards over time.\n",
    "\n",
    "In essence, actions that previously produced favorable outcomes become increasingly probable, gradually optimizing the policy to maximize long-term rewards.\n",
    "\n",
    "So, concerning a trajectory and an initial state $s_0$, we need a way to compute the gradient of the expected total reward from state $s_0$ with respect to the parameters. The policy gradient theorem states that:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\pi} \\left[ G_0 \\mid s_0  \\right] \\propto \\sum\\limits_{s} \\mu(s) \\sum\\limits_{s} Q(s, a) \\nabla \\pi(a \\mid s; \\mathbf{w})\n",
    "$$\n",
    "\n",
    "On which we use log probabilities to stabilize training by converting multiplicative probability updates into additive updates\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\mathbb{E}_{\\pi} \\left[ \\nabla_{\\mathbf{w}} \\log \\pi(a \\mid s; \\mathbf{w}) Q(s, a)\\right]\n",
    "$$\n",
    "\n",
    "where $\\mu(s)$ is the probability of being in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "REINFORCE is a Monte Carlo algorithm which is defined by the following steps\n",
    "\n",
    "1. Compute episodic trajectories $(s_0, a_0, r_1, s_1, a_1, s_2, \\dots)$\n",
    "2. Discounted rewards\n",
    "$$\n",
    "R_t = \\sum\\limits_{k=t}^{T} \\gamma^{k-t} r_k\n",
    "$$\n",
    "3. Compute the policy gradient and update the parameters\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\sum\\limits_{t=0}^{T} R_t \\nabla_{\\mathbf{w}} \\log \\pi(a_t \\mid s_t; \\mathbf{w})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient and continuous action space\n",
    "\n",
    "#### [Car Racing](https://gymnasium.farama.org/environments/box2d/car_racing/)\n",
    "\n",
    "The reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles visited in the track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True)\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "def policy(state: int):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "end_episode = False \n",
    "max_count = 100\n",
    "total_reward = 0\n",
    "while not end_episode:\n",
    "    if max_count < 0:\n",
    "        break \n",
    "    action = policy(state)\n",
    "    s_prime, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    end_episode = terminated or truncated\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "    print(f\"Action: {action} - Total Reward: {total_reward}\")\n",
    "    #time.sleep(.1)\n",
    "    state = s_prime\n",
    "    max_count -= 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce implementation\n",
    "We use an Advantage function to reduce variance, normalize rewards and perform batch updates. See [REINFORCE](./gymbase/reinforce.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymbase.reinforce import PolicyGradientAgent, preprocess_frame\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "GAMMA = 0.99 \n",
    "EPISODES = 3000\n",
    "BATCH_SIZE = 10\n",
    "HIDDEN_UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have continuous actions (steering, acceleration, braking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, agent: PolicyGradientAgent, episodes: int):\n",
    "\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(episodes), total=episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, term, trunc, _ = env.step(action)\n",
    "            next_state = preprocess_frame(next_state)\n",
    "            agent.store_outcome(log_prob, reward, state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            done = term or trunc\n",
    "            if done:\n",
    "                break\n",
    "        # Update policy every BATCH_SIZE episodes\n",
    "        if episode % BATCH_SIZE == 0:\n",
    "            agent.update_policy()\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: training this architecture on Car Racing is super slow even on CUDA. The following code is just a snipped for showing the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=100)\n",
    "\n",
    "action_dim = env.action_space.shape[0] \n",
    "agent = PolicyGradientAgent(action_dim)\n",
    "\n",
    "EPISODES = 10\n",
    "\n",
    "rewards = train(env=env, agent=agent, episodes=EPISODES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlen, mtime, mret = env.length_queue, env.time_queue, env.return_queue\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4), ncols=4, nrows=1)\n",
    "sns.lineplot(x=range(len(mlen)), y=mlen, ax=ax[0], label=\"REINFORCE\")\n",
    "sns.lineplot(x=range(len(mtime)), y=mtime, ax=ax[1], label=\"REINFORCE\")\n",
    "sns.lineplot(x=range(len(mret)), y=mret, ax=ax[2], label=\"REINFORCE\")\n",
    "sns.lineplot(x=range(len(rewards)), y=rewards, ax=ax[3], label=\"REINFORCE\")\n",
    "ax[0].set_title(\"Episode Lenght\")\n",
    "ax[1].set_title(\"Time of Visit\")\n",
    "ax[2].set_title(\"Return\")\n",
    "ax[3].set_title(\"Total Rewards\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "state, info = env.reset()\n",
    "state = preprocess_frame(state)\n",
    "\n",
    "end_episode = False \n",
    "max_count = 1000\n",
    "total_reward = 0\n",
    "while not end_episode:\n",
    "    if max_count < 0:\n",
    "        break \n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        mean, _ = agent.policy(state_tensor)\n",
    "    action = mean.cpu().numpy().squeeze()\n",
    "    s_prime, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    end_episode = terminated or truncated\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "    print(f\"Action: {action} - Total Reward: {total_reward}\")\n",
    "    #time.sleep(.1)\n",
    "    state = preprocess_frame(s_prime)\n",
    "    max_count -= 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
