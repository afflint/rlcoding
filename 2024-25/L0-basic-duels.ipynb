{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### UniversitÃ  degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# Duels\n",
    "\n",
    "## A fantasy game for reinforcement learning\n",
    "\n",
    "### Alfio Ferrara, Luigi Foscari\n",
    "\n",
    "In **Duels** an autonomous agent fights a unlimited number of duels against other agents to score victory points. The game can be played in different versions depending on the reinforcement learning problem being addressed. For example, it can be played by a single agent learning against fictitious opponents in an MDP, it can be limited to a predefined number of duels (finite horizon), or it can involve autonomous agents competing against each other while learning their own game strategies (MARL).\n",
    "\n",
    "The set of common rules for the game are described in the following section, followed by specific rules for other settings.\n",
    "\n",
    "## Base Game\n",
    "\n",
    "A game of Duels is a sequence of fights that may end with a **victory**, a **retreat**, or the **death** of the hero (the Agent). In case of death, the game ends immediately. In case of a retreat, the hero loses victory points (VP) but can immediatly engage a new duel against a weaker opponent. In case of a win, the hero gains victory points (VP) and immediatly engages a new duel against a stronger opponent.\n",
    "\n",
    "#### The duel\n",
    "\n",
    "A single duel is composed by a sequence of rounds. In each round, each duelist **performs an action**. Each **action can either succeed or fail**. If it succeeds, it has **an effect on the opponent in terms of hit points** (HP). The **effect depends on the action chosen by the opponent**, as specified in the following table. **If it fails, there is no effect**. In a **base game version** the action is always a success and the outcome of the action in terms of the HP loss for the opponent depends on the action chosen by the two players as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OrderEnforcing' object has no attribute 'EFFECTIVENESS_TABLE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuels-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, starting_hp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, opponent_distr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEFFECTIVENESS_TABLE\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OrderEnforcing' object has no attribute 'EFFECTIVENESS_TABLE'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymbase.environments\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"Duels-v0\", starting_hp=20, opponent_distr=None)\n",
    "observation, info = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Duels-v0\", starting_hp=20, opponent_distr=None)\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(f\"Agent starts with {observation['agent']} hit points\")\n",
    "print(f\"Opponent starts with {observation['opponent']} hit points\\n\")\n",
    "\n",
    "end_episode = False\n",
    "while not end_episode:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"Agent uses {info['agent']} and opponent uses {info['opponent']}\")\n",
    "    print(f\"Agent now has {observation['agent']} HP and opponent has {observation['opponent']} HP\\n\")\n",
    "\n",
    "    if truncated:\n",
    "        print(\"They decided that today was not a good day to fight\")\n",
    "    elif terminated:\n",
    "        if observation['agent'] <= 0 and observation['opponent'] <= 0:\n",
    "            print(\"The hero died facing the evil threat\")\n",
    "        elif reward > 0:\n",
    "            print(\"The hero vanquished evil\")\n",
    "        elif reward < 0:\n",
    "            print(\"The evil prevailed\")\n",
    "        \n",
    "    end_episode = terminated or truncated\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
