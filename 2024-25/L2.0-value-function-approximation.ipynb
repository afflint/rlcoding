{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Università degli Studi di Milano, Data Science and Economics Master Degree\n",
    "\n",
    "# VFA\n",
    "## Value Function Approximation\n",
    "\n",
    "### Alfio Ferrara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Reinforcement Learning methods are based on the idea of updating the value $V(s)$ of the states in which the agent can find itself or, alternatively, the value $Q(s, a)$, which represents the value assigned to taking an action $a$ when in state $s$.\n",
    "\n",
    "In **tabular solutions**, the functions $V(s)$ and $Q(s, a)$ are explicitly represented as tables that associate a value with states and state-action pairs.\n",
    "\n",
    "However, maintaining $V(s)$ and $Q(s, a)$ explicitly is **not practical** when the number of states becomes significantly large, even if finite. Some examples:\n",
    "\n",
    "- The possible configurations of the **Rubik’s Cube** are approximately $4.3 \\times 10^{19}$.  \n",
    "- The number of possible **positions in Chess** is around $10^{47}$.  \n",
    "- In a **real-world map**, even if represented as a discrete grid, there can be **billions of different positions**.\n",
    "\n",
    "Moreover, there are several real world situations where the number of states is simply **infinite**, for example because we have a continuous state space. Examples:\n",
    "- In **autonomous vehicle control** the veihicle state is typically represented by continuous variables, such as **speed**, **position** as $(x, y)$ coordinates, **steering angle**, etc.\n",
    "- If we need to **control the temperature** in a room, we typically describe the state as a combination of **temperature**, **humidity**, **number of people in the room**, etc.\n",
    "- In a **real-world map**, instead of a large grid, we may want to represent the position of agents just through their coordinates **(longitude, latitude)**.\n",
    "\n",
    "### A matter of generalization\n",
    "However, the number of states is not the only limitation of tabular solutions. There is also another relevant issue that is associated with the information that the agent can exploit about a state. Let's see a couple of examples:\n",
    "\n",
    "![](./imgs/obs-space.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **First example**: here we have a discrete state space. However, with respect to the target of reaching the apple, the situation of the pig (the agent) is identical. Thus, we expect $V(s_A) \\sim V(s_B)$, but the two states are distinct in a tabular setting. This means that when the Agent reaches $s_B$ it cannot exploit what has been learned about $V(s_A)$ in order to estimate $V(s_B)$.\n",
    "- **Second example**: the position of the space probe (the agent) is almost the same in $s_A$ and $s_B$. Thus, we would like to exploit the information available about $V(s_A)$ in order to choose the action to take when in $s_B$.\n",
    "\n",
    "To overcome this issue, we would like to have a mechanism for **generalizing the notion of state** and act similarly when we are in states that are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jousting Duel: A Tactical Medieval Jousting Game\n",
    "As an example of such an environment, we introduce Jousting Duel. Jousting Duel is a reinforcement learning-driven medieval jousting game where **two knights charge toward each other in a high-speed duel**. The player controls one knight, making **strategic decisions** on **steering**, **speed**, and **lance positioning** to **maximize their chances of landing a successful hit** while avoiding their opponent’s attack.\n",
    "\n",
    "The game features a continuous state space, where the knight’s position, speed, and lance angle are dynamically updated. However, the player can only take discrete actions, such as steering left or right and adjusting their lance angle. Success depends on precise timing and positioning, requiring the agent to learn optimal strategies through reinforcement learning.\n",
    "\n",
    "The goal is to hit the opponent's weak spot while dodging their lance. The game rewards efficient jousting techniques and penalizes missed attacks or poor positioning. \n",
    "\n",
    "<img style=\"width: 50%;\" src=\"./imgs/jousting-duel.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech notes\n",
    "#### Opponent (aka Environment) parameters\n",
    "- `max_distance = 10.0` : starting distance between opponents\n",
    "- `min_distance = 0.0` : collision point\n",
    "- `speed_agent = speed_opponent = 1.0` : fixed speed for agent and opponent\n",
    "- `lance_angle_change = 0.1` : Change in lance angle per action\n",
    "- `opponent_lance_angle = np.random.uniform(-1, 1)` : random lance angle per episode\n",
    "\n",
    "#### State space\n",
    "- $\\delta_t$: **relative distance** of opponents at time $t$ in $[\\textrm{max distance}, 0]$\n",
    "- $\\sigma$: **relative speed** of the two opponents in $[-5, 5]$\n",
    "- $\\theta$: **agent lance angle** in $[-1, 1]$\n",
    "- $\\phi$: **opponent lance angle** in $[-1, 1]$\n",
    "\n",
    "#### Action space\n",
    "With $c$ as a constant steer rate:\n",
    "- $a=0$: **steer left** $\\rightarrow \\theta = \\theta - c$\n",
    "- $a=1$: **stay centered** $\\rightarrow \\theta = \\theta$\n",
    "- $a=1$: **steer right** $\\rightarrow \\theta = \\theta + c$\n",
    "- $a=2$: **steer left** $\\rightarrow \\theta = \\theta - c$\n",
    "- $a=3$: **lance up** $\\rightarrow \\theta = \\theta + c$\n",
    "- $a=4$: **lance down** $\\rightarrow \\theta = \\theta - c$\n",
    "- $a=5$: **increase speed** $\\rightarrow \\sigma = \\min{(\\sigma + 1; 5)}$\n",
    "\n",
    "#### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymbase import environments\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken 3\n",
      "\n",
      "        Distance: 8.00, \n",
      "        Agent Lance Angle: 0.10, \n",
      "        Opponent Lance Angle: 0.60, \n",
      "        Distance to target: 0.50\n",
      "\n",
      "Action taken 3\n",
      "\n",
      "        Distance: 6.00, \n",
      "        Agent Lance Angle: 0.20, \n",
      "        Opponent Lance Angle: 0.60, \n",
      "        Distance to target: 0.40\n",
      "\n",
      "Action taken 4\n",
      "\n",
      "        Distance: 4.00, \n",
      "        Agent Lance Angle: 0.10, \n",
      "        Opponent Lance Angle: 0.60, \n",
      "        Distance to target: 0.50\n",
      "\n",
      "Action taken 1\n",
      "\n",
      "        Distance: 2.00, \n",
      "        Agent Lance Angle: 0.10, \n",
      "        Opponent Lance Angle: 0.60, \n",
      "        Distance to target: 0.50\n",
      "\n",
      "Action taken 0\n",
      "\n",
      "        Distance: 0.00, \n",
      "        Agent Lance Angle: 0.00, \n",
      "        Opponent Lance Angle: 0.60, \n",
      "        Distance to target: 0.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('JoustingDuel-v0', distance=10)\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    print(f\"Action taken {action}\")\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    env.render()  # Print the game state\n",
    "    print()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
