{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recycling Robot\n",
    "> Sutton, R. S., & Barto, A. G. (2018).Â Reinforcement learning: An introduction. MIT press. (Example 3.3)\n",
    "\n",
    "![](imgs/recycling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.model import MDP, Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta, r_wait, r_search = .8, .9, -1, 1 \n",
    "conf = [\n",
    "    Transition(0, 0, 0, 1, r_wait),\n",
    "    Transition(0, 1, 0, alpha, r_search),\n",
    "    Transition(0, 1, 1, 1 - alpha, r_search),\n",
    "    Transition(1, 1, 1, beta, r_search),\n",
    "    Transition(1, 1, 0, 1 - beta, -3),\n",
    "    Transition(1, 0, 1, 1, r_wait),\n",
    "    Transition(1, 2, 0, 1, 0),\n",
    "]\n",
    "\n",
    "mdp = MDP(2, 3, config=conf, gamma=1)\n",
    "mdp.set_state_names(HIGH=0, LOW=1)\n",
    "mdp.set_action_names(wait=0, search=1, recharge=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  state    action s_prime  probability  reward\n",
      "0  HIGH      wait    HIGH          1.0    -1.0\n",
      "1  HIGH    search    HIGH          0.8     1.0\n",
      "2  HIGH    search     LOW          0.2     1.0\n",
      "3   LOW      wait     LOW          1.0    -1.0\n",
      "4   LOW    search    HIGH          0.1    -3.0\n",
      "5   LOW    search     LOW          0.9     1.0\n",
      "6   LOW  recharge    HIGH          1.0     0.0\n"
     ]
    }
   ],
   "source": [
    "table = mdp.to_table()\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.policy import Policy\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wait': 0, 'search': 1, 'recharge': 2}\n",
      "{'HIGH': 0, 'LOW': 1}\n"
     ]
    }
   ],
   "source": [
    "print(mdp.action_id)\n",
    "print(mdp.state_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchPolicy(Policy):\n",
    "    def __getitem__(self, state: int) -> int:\n",
    "        return 1\n",
    "\n",
    "class SearchWaitPolicy(Policy):\n",
    "    def __getitem__(self, state: int) -> int:\n",
    "        if state == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SearchPolicy(mdp)\n",
    "sw_policy = SearchWaitPolicy(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation\n",
    "\n",
    "$$\n",
    "V_t(s) = Q_{t-1}(s, \\pi_s)\n",
    "$$\n",
    "$$\n",
    "Q_{t-1}(s, \\pi_s) = \\sum\\limits_{s'} T(s, a, s')\\left[ reward(s, \\pi_s, s') + \\gamma  V_{t-1}(s')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.algorithms import policy_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[734.22222222 732.88888889]\n",
      "[ -990. -1000.]\n"
     ]
    }
   ],
   "source": [
    "print(policy_evaluation(policy=search, mdp=mdp))\n",
    "print(policy_evaluation(policy=sw_policy, mdp=mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration\n",
    "$$\n",
    "    V^{*}_{t(s)} = \\max\\limits_a Q^{*}_{t-1}(s, a)\n",
    "$$\n",
    "$$\n",
    "    Q^{*}_{t-1}(s, a) = \\sum\\limits_{s'} T(s, a, s')\\left[reward(s, a, s') + \\gamma  V^{*}_{t-1}(s')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.algorithms import value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_star, Pi_star, value_history, policy_history = value_iteration(mdp=mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[833.61222222 832.77888889]\n",
      "HIGH search\n",
      "LOW recharge\n"
     ]
    }
   ],
   "source": [
    "print(V_star)\n",
    "for s, a in Pi_star.items():\n",
    "    print(\"{} {}\".format(mdp.states[s], mdp.actions[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration\n",
    "\n",
    "When we have a policy $\\pi$, for all states $s$, we can find a greedy policy $\\pi'$ by:\n",
    "$$\n",
    "\\pi'(s) = \\arg\\max\\limits_{a} \\sum\\limits_{s'}p(s'\\mid s, a) \\left [ r(s, a, s') + \\gamma V_{\\pi}(s')\\right ]\n",
    "$$\n",
    "**Note**: suppose that $\\pi'$ is not better than the old policy $\\pi$. Then $V_{\\pi} = V_{\\pi'}$. Thus we can rewrite:\n",
    "$$\n",
    "V_{\\pi'}(s) = \\max\\limits_{a} \\sum\\limits_{s'}p(s'\\mid s, a) \\left [ r(s, a, s') + \\gamma V_{\\pi'\n",
    "}(s')\\right ]\n",
    "$$\n",
    "Now, since (7) is the **Bellman optimality equation** $V_{\\pi'}$ must be $V^*$ and both $\\pi$ and $\\pi'$ must be optimal policies. Thus policy improvement **always gives us a strictly better policy except when the policy is already optimal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp.algorithms import policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, history = policy_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGH search\n",
      "LOW recharge\n"
     ]
    }
   ],
   "source": [
    "for s in mdp.states:\n",
    "    print(\"{} {}\".format(mdp.states[s], mdp.actions[pi[s]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({0: 0, 1: 1}, array([-1000.,  -984.])),\n",
       " ({0: 1, 1: 1}, array([734.22222222, 732.88888889])),\n",
       " ({0: 1, 1: 2}, array([833.47222222, 832.63888889])),\n",
       " ({0: 1, 1: 2}, array([833.47222222, 832.63888889]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
