{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24f4af5",
   "metadata": {},
   "source": [
    "# Quick overview of gymnasium: Q-learning and SARSA\n",
    "[Gymnasium website](https://gymnasium.farama.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f8624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbea652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcd478",
   "metadata": {},
   "source": [
    "## Taxi driver\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, drives to the passenger’s destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "**Actions**\n",
    "- 0: move south;\n",
    "- 1: move north;\n",
    "- 2: move east;\n",
    "- 3: move west;\n",
    "- 4: pickup the passenger;\n",
    "- 5: dropoff the passenger.\n",
    "\n",
    "4 locations $\\times$ 5 passenger locations (including the taxi) $\\times$ 25 grid positions = 500 space\n",
    "\n",
    "Each action takes 1 point meaning a reward of -1; successfully delivering the passenger yields +20; any illegal move costs -10 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9ae3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0819f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6b7d7",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a0f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render_modes': ['human', 'ansi', 'rgb_array'], 'render_fps': 4}\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n",
      "1\n",
      "(124, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "print(env.env.metadata)\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "random_action = env.action_space.sample()\n",
    "print(random_action)\n",
    "print(env.step(random_action))\n",
    "# next_state, reward, terminated, truncated , info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39e491",
   "metadata": {},
   "source": [
    "## Example: use gym to implement Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2528f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfe0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode='ansi')\n",
    "\n",
    "# Initialize the q-table with zero values\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.1  # learning-rate\n",
    "gamma = 0.7  # discount-factor\n",
    "epsilon = 0.1  # explor vs exploit\n",
    "max_steps = 100\n",
    "sleep_time = .05\n",
    "\n",
    "# Random generator\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for i in range(1000):\n",
    "    state = env.reset()[0]\n",
    "    clear_output(wait=True)\n",
    "    print(env.render())\n",
    "    time.sleep(sleep_time)\n",
    "    terminated = False\n",
    "    \n",
    "    for j in range(max_steps):\n",
    "        if rng.random() < epsilon:\n",
    "            action = env.action_space.sample() # Explore the action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        # Apply the action and see what happens\n",
    "        next_state, reward, terminated, truncated, info = env.step(action) \n",
    "        \n",
    "        q_table[state, action] = (1 - eta) * q_table[state, action] + eta * (\n",
    "            reward + gamma * np.max(q_table[next_state]))\n",
    "        if terminated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6c24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crike",
   "language": "python",
   "name": "crike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
